\documentclass{article}
\usepackage{multicol}
\usepackage[top=1mm,bottom=1mm,left=1mm,right=1mm]{geometry}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{etexcmds}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}

\renewcommand{\S}{\mathcal{S}}
\renewcommand{\P}{\mathcal{P}}
\providecommand{\B}{\mathcal{B}}
\providecommand{\G}{\mathcal{G}}
\providecommand{\F}{\mathcal{F}}
\providecommand{\A}{\mathbb{A}}
\providecommand{\X}{\mathbb{X}}
\providecommand{\Y}{\mathbb{Y}}
\providecommand{\Z}{\mathbb{Z}}
\providecommand{\W}{\mathbb{W}}
\providecommand{\N}{\mathbb{N}}
\providecommand{\R}{\mathbb{R}}
\providecommand{\C}{\mathbb{C}}

\begin{document}
    \raggedright
    \small
    \setlength{\columnseprule}{0.4pt}

    \begin{multicols*}{3}

        \textbf{DeMorgan's Laws:}\\
        \begin{itemize}
            \item $\overline{A\cap B} = \overline{A} \cup \overline{B}$\\
            \item $\overline{A\cup B} = \overline{A} \cap \overline{B}$
        \end{itemize}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Countable} if elements can be put in one-to-one with $\N = \{1,2,3,\dots\}$

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Uncountable sets:}
        \begin{itemize}
            \item $\R = (-\infty, \infty)$
            \item $[0, 1] \text{ and } (0,1)$
            \item $(a,b), \forall a,b\in \R \text{ such that } a<b$
        \end{itemize}

        \noindent\rule{\columnwidth}{0.4pt}

        A family of sets $\{A_i,i\in I\}$ is a \textbf{partition of $\S$} if it is disjoint and collectively exhaustive over $\S$.

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Axioms of Probability}
        \begin{enumerate}
            \item $\P(A)\geq0,\allowbreak\forall A\in\F(\S))$
            \item $\P(\S)=1$
            \item $\P(\cup_{i=1}^{\infty} A_i) = \sum_{i=1}^{\infty} \P( A_i)$
        \end{enumerate}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Probability Equations}
        \begin{enumerate}
            \item $\P(A\cup B) = \P(A)+\P(B)-\P(A\cap B)$
            \item $\P(A|B)=\frac{\P(A\cap B)}{\P(B)}=\frac{\P(B|A)\P(A)}{\P(B)}$
        \end{enumerate}

        \noindent\rule{\columnwidth}{0.4pt}

        Given $\S$ and a family of subsets $\G=\{A_i, i\in I\}$ of $\S$, the \textbf{$\sigma$-field generated by $\G$}, denoted $\sigma(\G)$, is the smallest $\sigma$ field containing all the subsets in $\G$.

        By ``smallest'' $\sigma$-field, we mean that for any $\sigma$-field $\F_0$ containing all the sets in $\G$:
        \begin{equation*}
            \sigma(\G)\subset\F_0
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        Given $\R$, the \textbf{Borel field of $\R$} is defined as the $\sigma$-field generated by the family of all open intervals
        \begin{equation*}
            \G=\{(a,b):\forall(a,b)\in\R \text{ such that }a < b\}.
        \end{equation*}
        We denote the Borel field of $\R$ by $\B(\R)$.

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{pmfs}

        \textbf{uniform}\\
        $p(\omega)=\frac{1}{n},\forall\omega\in\S$\\

        \textbf{binomial}\\
        $p(k)=\allowbreak{n\choose k}a^k(1-a)^{n-k}$\\

        \textbf{geometric}\\
        $p(k)=(1-a)a^k,a\in(0,1)$\\

        \textbf{poisson}\\
        $p(k)=\frac{\lambda^{k}e^{-k}}{k!};k=0,1,2,\dots;\lambda>0$

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Properties of the pdf:}
        \begin{enumerate}
            \item $f(r)\geq0,\forall r\in\R$
            \item $\int_{-\infty}^{\infty} f(r)dr=1$
        \end{enumerate}
        Given a valid pdf, we can get a valid probability measure $\P(\cdot)$:
        \begin{equation*}
            \P(A)=\int_{A}f(r)dr=\int_{-\infty}^{\infty}\cdot1_A(r)dr
        \end{equation*}
        where
        \begin{equation*}
            1_A(r) =
            \begin{cases}
                1,r\in A\\
                0,r\notin A
            \end{cases}
        \end{equation*}
        is called the \textbf{indicator function} of the set A\@.

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{pdfs}

        \textbf{uniform}\\
        $f(r)=\frac{1}{b-a}1_{[a,b]}(r)$\\

        \textbf{exponential}\\
        $f(r)=\lambda e^{-\lambda r}\cdot 1_{[0,\infty]}(r),\lambda>0$\\
        $\hookrightarrow =
        \begin{cases}
            \lambda e^{-\lambda r},r\geq 0\\
            0,r<0
        \end{cases}$

        \textbf{gaussian}\\
        $f(r)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(\frac{-(r-\mu)^2}{2\sigma^2}\right), r\in\R,\mu\in\R,\sigma>0$

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Conditional Probability}
        \begin{equation*}
            \P(A|B)=\frac{\P(A\cap B)}{\P(B)}
        \end{equation*}

        \textbf{Bayes Formula}
        \begin{equation*}
            \P(A|B)=\frac{\P(B|A)\P(A)}{\P(B)}
        \end{equation*}

        \textbf{Total Probability Law}
        \begin{equation*}
            \P(B)=\sum_{i=1}^{n}\P(B|A_i)\P(A_i)
        \end{equation*}

        \textbf{Bayes Theorem}
        \begin{equation*}
            \P(A_m|B)=\frac{\P(B|A_m)\P(A_m)}{\sum_{i=1}^{n}\P(B|A_i)\P(A_i)}
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        Events $A$ and $B$ are \textbf{statistically independent} if and only if
        \begin{equation*}
            \P(A\cap B) = \P(A)\P(B)
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Bernoulli Trials}
        \begin{equation*}
            p_n(k)=\P(\B_k)={n\choose k}p^k(1-p)^{n-k}
        \end{equation*}
        where $p=\P_0(A)$.

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Properties of the cdf}
        \begin{equation*}
            F_{\X}(\alpha)=\P_x((-\infty,\alpha))=\P(\{\X\leq\alpha\})
        \end{equation*}
        where
        \begin{equation*}
            \{\X\leq\alpha\} = \{\omega\in\S:\X(\omega)\leq\alpha\}\in\F
        \end{equation*}
        Properties:
        \begin{enumerate}
            \item $F_\X(\infty)=1$ and $F_\X(-\infty)=0$
            \item If $x_1<x_2$, then $F_\X(x_1)\leq F_\X(x_2)$
            \item $\P(\{\X>\alpha\}) = 1-F_\X(\alpha)$
            \item If $x_1<x_2$, then $\P(\{x_1<\X\leq x_2\}) = F_\X(x_2)-F_\X(x_1)$
            \item $\P(\{\X=x_0\}) = F_\X(x_o)-F_\X(x_0^-)$ where $F_\X(x_0^-)=\lim_{\epsilon\downarrow0}F_\X(x_0-\epsilon)$
        \end{enumerate}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Properties of the pdf}
        \begin{equation*}
            f_{\X}(x)=\frac{dF_\X(x)}{dx}
        \end{equation*}
        Properties:
        \begin{enumerate}
            \item $f_\X(x)\geq0,\forall x\in\R$
            \item $F_\X(x)=\int_{-\infty}^{x} f_\X(\alpha)d\alpha $
            \item $\int_{\infty}^{\infty} f_\X(x)dx = F_\X(\infty) - F_\X(-\infty) = 1$
            \item $\P(\{x_1<\X\leq x_2\}) =\int_{x_1}^{x_2}f_\X(x)dx=F_\X(x_2)-F_\X(x_1) $
        \end{enumerate}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Dirac $\delta$-functions:} $\delta(x)$

        \begin{itemize}
            \item $\delta(x)=0,\forall x\neq0$
            \item $\int_{\infty}^{\infty} \delta(x)dx=\int_{-\epsilon}^{\epsilon}\delta(x)dx=1,\forall\epsilon>0 $
        \end{itemize}

        \textbf{Sifting Property of Dirac $\delta$-functions}
        \begin{equation*}
            \int_{\infty}^{\infty} g(x)\delta(x-x_0)dx=g(x_0)
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Random Variable Forms}

        \textbf{gaussian}
        \begin{equation*}
            f_\X(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp[\frac{-(x-\mu)^2}{2\sigma^2}],\forall x\in\R
        \end{equation*}
        n.b.
        \begin{equation*}
            \begin{gathered}
                F_\X(x)=\int_{-\infty}^{x}f_{\X}(\alpha)d\alpha=\Phi(\frac{x-\mu}{\sigma})\\
                \text{where } \Phi(r)=\int_{-\infty}^{r} \frac{1}{\sqrt{2\pi}}e^{\frac{-z^2}{2}}dz
            \end{gathered}
        \end{equation*}
        So if $\X$ is a Gaussian RV with parameters $\mu\in\R$ and $\sigma>0$, then
        \begin{equation*}
            \P(\{a<\X\leq b\}) = \Phi(\frac{b-\mu}{\sigma}) - \Phi(\frac{a-\mu}{\sigma})
        \end{equation*}

        \textbf{uniform}
        \begin{equation*}
            \begin{gathered}
                \X=\text{u}[a,b], a<b\\
                f_\X(x)=\frac{1}{b-a}\cdot 1_{[a,b]}(x)\\
                F_{\X}(x)=\int_{\infty}^{x} f_{\X}(\alpha)d\alpha
            \end{gathered}
        \end{equation*}

        \textbf{exponential}\\
        An RV $\X$ with pdf of the form
        \begin{equation*}
            f_{\X}(x)=\alpha e^{-\alpha x}\cdot 1_{[0,\infty]}(x)
        \end{equation*}
        where $\alpha>0$ is called an exponential RV with parameter $\alpha$.
        \begin{equation*}
            F_{\X}(\alpha) = \int_{-\infty}^{\alpha} f_{\X}(x)dx
        \end{equation*}

        \textbf{binomial}\\
        Discrete RV taking on values in the set $\{0,1,2,\cdots,n\}\subset\R$ with pmf
        \begin{equation*}
            \begin{gathered}
                \P_{\X}(k)={n\choose k}p^k(1-p)^{n-k},\\
                F_{\X}(x)= \sum_{k=0}^{m(x)} {n\choose k}p^k(1-p)^{n-k}\\
                \text{where } m(x)\leq x< m(x)+1,\\
                f_{\X}(x) = \sum_{k=0}^{n} {n\choose k}p^k(1-p)^{n-k}\delta(x-k)
            \end{gathered}
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Conditional cdf and pdf}
        \begin{equation*}
            \begin{gathered}
                F_{\X}(x|M)=\P(\{\X\leq x\}|M) = \frac{\P(\{\X\leq x\}\cap M)}{\P(M)}\\
                f_{\X}(x|M) = \frac{dF_\X(x|M)}{dx}
            \end{gathered}
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Total Probability Law}
        \begin{equation*}
            \begin{gathered}
                F_{\X}(x)=F_{\X}(x|A_1)P(A_1)+\cdots+F_{\X}(x|A_n)P(A_n)\\
                f_{\X}(x)=f_{\X}(x|A_1)P(A_1)+\cdots+f_{\X}(x|A_n)P(A_n)
            \end{gathered}
        \end{equation*}

        \textbf{Bayes Formula}
        \begin{equation*}
            \begin{gathered}
                \P(A|\{\X\leq x\}) = \frac{F_\X(x|A)\P(A)}{F_\X(x)}
            \end{gathered}
        \end{equation*}

        \newpage
        Now consider $\P(A|B) = \frac{\P(B|A)\P(A)}{\P(B)}$ when $B=\{x_1<\X\leq x_2\}$:
        \begin{equation*}
            \begin{gathered}
                \P(A|\{x_1<\X\leq x_2\}) = \\
                \hookrightarrow = \frac{F_\X (x_2|A) - F_\X (x_1|A)\P(A)}{F_\X(x_2) - F_\X(x_1)}
            \end{gathered}
        \end{equation*}

        For $\X=x$,
        \begin{equation*}
            \begin{gathered}
                \P(A|\{\X=x\}) = \frac{f_\X(x|A)}{f_\X(x)}\P(A)\\
                \P(A)= \int_{-\infty}^{\infty} \P(A|\{\X=x\})f_{\X}dx
            \end{gathered}
        \end{equation*}

        \textbf{Bayes' Theorem}
        \begin{equation*}
            f_{\X}(x|A) = \frac{\P(A|\{\X=x\})f_\X(x)}{\int_{-\infty}^{\infty}\P(A|\{\X=\alpha\})f_\X(\alpha)d\alpha }
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Functions of RVs}\\
        For $\Y=g(\X)$ to be measurable, $g(\cdot)$ must satisfy:
        \begin{enumerate}
            \item The domain of $g(\cdot)$ must contain the range space of $\X$.
            \item For each $y\in\R$, the set $R_y = \{x\in\R:g(x)\leq y\}$ must be a Borel set.
            \item The events $\{g(\X)=\pm\infty\}$ must have probability 0.
        \end{enumerate}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Direct pdf Method}\\
        Suppose $\Y=g(\X)$, where $G:\R\rightarrow\R$ such that the inverse $g^{-1}(\cdot)$ exists, and assume that
        \begin{equation*}
            \frac{dx}{dy} = \frac{dg^{-1}(y)}{dy}
        \end{equation*}
        exists.
        Then
        \begin{equation*}
            f_\Y(y) = f_\X(x(y)) \cdot \abs*{\frac{dx(y)}{dy}}
        \end{equation*}
        where $x(y)=g^{-1}(y)$.

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Integration by parts}
        \begin{equation*}
            \int u v' du = uv - \int v u' du
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        The \textbf{mean} or \textbf{expected value} of a RV $\X$ with pdf $f_\X(x)$ is
        \begin{equation*}
            E[\X] = \int_{-\infty}^{\infty}  xf_{\X}(x)dx
        \end{equation*}
        The definition above applies to discrete RVs if we write their pdf using $\delta$-functions:
        \begin{equation*}
            \begin{gathered}
                f_{\X}(x) = \sum_k p_\X(x_k)\delta(x-x_k) = \sum_{k}p_k\delta(x-x_k)\\
                E[\X] = \sum_k x_k p_{\X}(x_k) = \sum_k k\cdot p_{\X}(k)
            \end{gathered}
        \end{equation*}
        If $\Y=g(\X)$ is a RV, then
        \begin{equation*}
            E[\Y] = E[g(\X)] = \int_{\infty}^{\infty} g(x) f_{\X}(x)dx.
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Conditional mean of an RV}
        \begin{equation*}
            \begin{gathered}
                E[\X|M] = \int_{-\infty}^{\infty} xf_{\X}(x|M)dx \text{ (continuous)}\\
                E[\X|M] = \sum_k x_k p_{\X}(x_k|M) \text{ (discrete)}
            \end{gathered}
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Expected value of function $g(\X)$}
        \begin{equation*}
            E[g(\X)] = \int_{-\infty}^{\infty} g(x)f_{\X}(x)dx = \sum_k g(x_k)p_{\X}(x_k)
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Linearity of Expectation}\\
        Let $g_1(\X)$ and $g_2(\X)$ be two function of a RV $\X$ and let $\alpha$ and $\beta$ be two constant $(\alpha, \beta\in\R)$.
        Then
        \begin{equation*}
            E[\alpha g_1(\X) + \beta g_2(\X)] = \alpha E[g_1(\X)] + \beta E[g_2(\X)]
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Variance of RV $\X$}
        \begin{equation*}
            \text{var}(\X) = E[(\X - \overline{\X})^2] = \int_{-\infty}^{\infty}(x-\overline{\X})^2 f_\X(x)dx
        \end{equation*}
        where $\overline{\X} = E[\X]$.\\
        The positive square root of the variance of $\X$ is called the \textbf{standard deviation} of $\X$:
        \begin{equation*}
            \text{StDev}(\X) = \sigma_x = \sqrt[]{\text{var}(\X)}
        \end{equation*}
        n.b.:
        \begin{equation*}
            \text{var}(\X) = E[(\X - \overline{\X})^2] = E[\X^2] - (E[\X])^2
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Means and variances of RV types}\\
        \textbf{gaussian}\\
        $E[\X] = \mu$\\
        $\text{var}(\X) = \sigma^2$\\
        \textbf{poisson}\\
        $E[\X] = \mu$\\
        $\text{var}(\X) = \mu$

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{The Characteristic Function}
        \begin{equation*}
            \Phi_{\X}(\omega) = E[e^{i\omega\X}], \omega\in\R
        \end{equation*}
        If $f_\X(x)$ is the pdf of $\X$, then
        \begin{equation*}
            \begin{gathered}
                \Phi_{\X}(\omega) = \int_{-\infty}^{\infty} e^{i\omega x}f_\X(x)dx\\
                \Phi_{\X}(\omega): \R\rightarrow\C
            \end{gathered}
        \end{equation*}
        Euler's formula:
        \begin{equation*}
            \begin{gathered}
                e^{i\omega x} = \cos(\omega x) + i\sin(\omega x)\\
                e^{i\pi} + 1 = 0
            \end{gathered}
        \end{equation*}
        If $\X$ is a RV with $\Phi_\X(\omega)$, and $\Y=a\X + b$ then
        \begin{equation*}
            \Phi_{\Y}(\omega) = e^{i\omega b}\Phi_{\X}(a\omega)
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Moment generating function}
        \begin{equation*}
            \phi_{\X}(s) = E[e^{s\X}] = \int_{-\infty}^{\infty}f_\X(x)e^{sx}dx
        \end{equation*}
        \textbf{Moment Theorem}\\
        Given a RV $\X$ with mgf $\phi_\X(s)$, the $n$-th moment of $\X$ is given by
        \begin{equation*}
            E[\X^n] = \frac{d^n\phi_\X(s)}{ds^n}\rvert_{s=0} = \phi_{\X}^{(n)}(0)
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        The \textbf{joint cdf} of two RVs is the probability of the event $\{\X\leq x\}\cap\{\Y\leq y\}$:
        \begin{equation*}
            F_{\X\Y}(x,y) = \P(\{\X\leq x\}\cap\{\Y\leq y\})
        \end{equation*}
        Properties:
        \begin{enumerate}
            \item $F_{\X\Y}(-\infty,y)=0$ and $F_{\X\Y}(x, -\infty)=0$,\\
            $F_{\X\Y}(\infty,y)=F_{\Y}(y)$ and $F_{\X\Y}(x, \infty)=F_{\X}(x)$,\\
            $F_{\X\Y}(\infty,\infty)=1$
            \item $\P(\{x_1<\X\leq x_2\}\cap\{\Y\leq y\}) = F_{\X\Y}(x_2,y) - F_{\X\Y}(x_1,y)$
            \item $\P(\{x_1<\X\leq x_2\}\cap\{y_1<\Y\leq y_2\}) = F_{\X\Y}(x_2,y_2) - F_{\X\Y}(x_2,y_1) $
        \end{enumerate}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Joint pdf}
        \begin{equation*}
            f_{\X\Y}(x,y) = \frac{\partial^{2}F_{\X\Y}(x,y)}{\partial x\partial y}
        \end{equation*}
        Properties:
        \begin{enumerate}
            \item $f_{\X\Y}(x,y)\geq 0$
            \item $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f_{\X\Y}(x,y)dxdy = 1$
            \item $\int_{-\infty}^{y} \int_{-\infty}^{x} f_{\alpha\beta}(x,y)d\alpha d\beta = F_{\X\Y}(x,y)$
            \item $\P(\{(\X,\Y)\in D\}) = \int_{}^{} \int_{\R^2} f_{\X\Y}(x,y)\cdot 1_D((x,y))dxdy$
        \end{enumerate}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Marginal pdfs}
        \begin{equation*}
            \begin{gathered}
                f_{\X}(x)=\int_{-\infty}^{\infty} f_{\X\Y}(x,y)dy\\
                f_{\Y}(y)=\int_{-\infty}^{\infty} f_{\X\Y}(x,y)dx
            \end{gathered}
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Jointly Gaussian RVs}
        \begin{equation*}
            \begin{gathered}
                f_{\X\Y}(x,y) = \frac{1}{2\pi \sigma_x \sigma_y \sqrt{1-r^2}}\cdot\\
                \exp{[\cdots\frac{(x-\mu_x)(y-\mu_y)}{\sigma_x \sigma_y}\cdots]}
            \end{gathered}
        \end{equation*}
        where $\mu_x,\mu_y\in\R$, $\sigma_x \sigma_y>0$, and $-1\leq r\leq1$.\\

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Statistically Independent RVs}\\
        Two RVs are statistically independent if the events $\{\X\in A\}$ and $\{\Y\in B\}$ are statistically independent for all $A,B\in \B(\R)$.
        \begin{equation*}
            \begin{gathered}
                F_{\X\Y}(x,y) = \P(\{\X\in A\}\cap\{\Y\in B\}) = F_{\X}(x)\cdot F_{\Y}(y)\\
                f_{\X\Y}(x,y) = f_{\X}(x)\cdot f_{\Y}(y)
            \end{gathered}
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        If $\X$ and $\Y$ are two j-dist, independent RVs then the pdf of their sum $\Z=\X+\Y$ is given by the convolution:
        \begin{equation*}
            \begin{gathered}
                f_{\Z}(z) = (f_\X * f_\Y)(z)\\
                f_{\Z}(z) = \int_{-\infty}^{\infty} f_{\Y}(y)f_{\X}(z-y)dy
            \end{gathered}
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Direct joint density determination}
        \begin{equation*}
            f_{\Z\W}(z,w)= f_{\X\Y}(x(z,w),y(z,w))\cdot\abs*{\frac{\partial (x,y)}{\partial (z,w)}}
        \end{equation*}
        where the Jacobian is the determinant
        \begin{equation*}
            \frac{\partial (x,y)}{\partial (z,w)} = \frac{\partial x}{\partial z}\cdot \frac{\partial y}{\partial w} - \frac{\partial y}{\partial z}\cdot \frac{\partial x}{\partial w}.
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Joint Moments}
        \begin{equation*}
            \begin{gathered}
                E[\Z] = E[g(\X,\Y)]\\
                = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}  g(x,y)f_{\X\Y}(x,y)dxdy
            \end{gathered}
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Correlation}
        \begin{equation*}
            \text{corr}(\X,\Y)=E[\X\Y]
        \end{equation*}
        \textbf{Covariance}
        \begin{equation*}
            \text{cov}(\X,\Y)=E[(\X-\overline{\X})(\Y-\overline{\Y})]
        \end{equation*}
        \textbf{Correlation coefficient}
        \begin{equation*}
            r_{\X\Y} = \frac{\text{cov}(\X,\Y)}{\sigma_x \sigma_y} = \frac{E[\X\Y]-E[\X]\cdot E[\Y]}{\sigma_x\sigma_y}
        \end{equation*}
        Two RVs are \textbf{uncorrelated} if their covariance is equal to zero.
        This is true if any of the following are true:
        \begin{enumerate}
            \item $\text{cov}(\X,\Y) = 0$
            \item $r_{\X\Y}=0$
            \item $E[\X\Y] = E[\X] \cdot E[\Y]$
        \end{enumerate}
        Two RVs are \textbf{orthogonal} if $E[\X\Y] = 0$.

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Joint characteristic functions}
        \begin{equation*}
            \begin{gathered}
                \Phi_{\X\Y}(\omega_1,\omega_2) = E[e^{i(\omega_1\X+\omega_2\Y)}]\\
                = \int \int e^{+i(\omega_1 x+\omega_2 y)}f_{\X\Y}(x,y)dxdy
            \end{gathered}
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Convolution Theorem}\\
        Let $\X$ and $\Y$ be two j-dist, statistically independent RVs and let $\Z=\X+\Y$, then
        \begin{equation*}
            \Phi_{\Z}(\omega) = \Phi_{\X}(\omega)\cdot\Phi_{\Y}(\omega) = E[e^{i\omega\X}]\cdot E[e^{i\omega\Y}]
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Joint moment generating function}
        \begin{equation*}
            \phi_{\X\Y}(s_1,s_2) = E[e^{s_1 \X + s_2 \Y}]
        \end{equation*}
        \textbf{Moment Theorem}
        \begin{equation*}
            E[\X^j\cdot\Y^k] = \phi_{\X\Y}^{(j,k)}(0,0)
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Conditional Distributions}
        \begin{equation*}
            \begin{gathered}
                F_{\X\Y}(x,y|M) = \P(\{\X\leq x\}\cap\{\Y\leq y\}|M)\\
                = \frac{\P(\{\X\leq x\}\cap\{\Y\leq y\}\cap M)}{\P(M)}
            \end{gathered}
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \begin{equation*}
            \begin{gathered}
                f_{\Y}(y|\{\X=x\}) = \frac{f_{\X\Y}(x,y)}{f_\X(x)}
            \end{gathered}
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Random Vectors}\\
        The vector of j-dist RVs
        \begin{equation*}
            \underline{\X} = (\X_1,\cdots,\X_n)
        \end{equation*}
        is a random vector.
        \begin{equation*}
            \begin{gathered}
                F_{\underline{\X}}(\underline{x}) = F_{\X_1\cdots\X_n}(x_1,\cdots,x_n)\\
                f_{\underline{\X}}(\underline{x}) = f_{\X_1\cdots\X_n}(x_1,\cdots,x_n)\\
            \end{gathered}
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Central Limit Theorem}\\
        Let $\{\X_n\}$ be a sequence of iid RVs with mean $\mu$ and variance $\sigma^2<\infty$.
        Define
        \begin{equation*}
            \Z_n = \frac{(\X_1 + \X_2 + \cdots + \X_n) - n\mu}{\sigma\sqrt{n}}
        \end{equation*}
        Then $\{Z_n\}$ converges in distribution to a RV $\Z$ that is Gaussian with mean 0 and variance 1.
        i.e.,
        \begin{equation*}
            F_{\Z_n}(z)\rightarrow\Phi(z)\text{ as }n\rightarrow\infty,\forall z\in\R
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Stochastic Processes}\\
        Instead of mapping each outcome $\omega\in\S$ to a number $\X(\omega)$, we map it to a function of time $\X(t,\omega)$.\\
        A \textbf{random process} is a family $\{\X(t); t\in\R\}$ of RVs defined on $(\S,\F,\P)$ and indexed by $t$.\\
        For a fixed outcome $\omega_0\in\S$, the time function $\X(\cdot) = \X(\cdot, \omega_0)$ is called the \textbf{sample path} of the random process $\X(t)$ corresponding to $\omega_0$.

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{$n$-th order stationary}\\
        A random process $\X(t)$ is called $n$-th order stationary if all $n$-th order cdfs or pdfs are invariant to time shifts in the origin.\\

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Strict-sense stationary (SSS)}\\
        A random process $\X(t)$ is called stationary, or SSS, if $\X(t)$ is $n$-th order stationary for all $n=1,2,3,\cdots$

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Wide-sense stationary (WSS)}\\
        A random process $\X(t)$ is called \textbf{WSS} if:
        \begin{enumerate}
            \item $E[\X(t)] = \overline{\X} = \text{constant}$
            \item $E[\X(t_1)\cdot \X(t_2)] = R(t_1 - t_2)$
        \end{enumerate}
        Note:
        \begin{enumerate}
            \item If a random process $\X(t)$ is first and second order stationary, and if $E[\X(t_1)]$ and $E[\X(t_1)\X(t_2)]$ exits, then $\X(t)$ is WSS. The converse is \textbf{not} true.
            \item If a RP $\X(t)$ is SSS, it is also WSS if $E[\X(t_1)]$ and $E[\X(t_1)\X(t_2)]$ exist.
            \item If $\X(t)$ is WSS, $E[\X(t_1)\X(t_2)]$, $\text{cov}(\X(t_1),\X(t_2))$ will depend only on the time difference $t_1 - t_2$.
        \end{enumerate}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Mean of RP $\X(t)$}
        \begin{equation*}
            \mu_{\X}(t) = E[\X(t)]
        \end{equation*}
        \textbf{Autocorrelation function}
        \begin{equation*}
            R_{xx}(t_1, t_2) = E[{\X(t_1)\X(t_2)}]
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Autocovariance function}
        \begin{equation*}
            C_{xx}(t_1,t_2) = E[(\X(t_1) - \mu_x(t_1))(\X(t_2) - \mu_x(t_2))]
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Power spectral density}\\
        If $\X(t)$ is WSS with autocorrelation function $R_x(\tau)$, then the \textbf{power spectral density} of $\X(t)$ is defined as
        \begin{equation*}
            S_{xx}(\omega) = \int_{-\infty}^{\infty} R_x(\tau)e^{-i\omega\tau}d\tau
        \end{equation*}
        $S_{xx}(\omega)$ is a measure of the average distribution of signal power in frequency for the RP $\X(t)$.
        This is the Fourier transform of the autocorrelation function for a WSS process.

        \noindent\rule{\columnwidth}{0.4pt}

        A linear system $L[\cdot]$ is a transformation rule satisfying the following two properties:
        \begin{enumerate}
            \item $L[\X_1(t)+\X_2(t)] = L[\X_1(t)] + L[\X_2(t)]$
            \item $L[\A\cdot\X(t)] = \A\cdot L[\X(t)]$
        \end{enumerate}
        A (linear) system is \textbf{time-invariant} if, given response $y(t)$ to input $x(t)$, it has response $y(t+c)$ for input $x(t+c)$ for all $c\in\R$.
        If we put a random process $\X(t)$ into an LTI system, we get a random process $\Y(t)$ out:
        \begin{equation*}
            \Y(t) = \X(t) * h(t) = \int_{-\infty}^{\infty} \X(t-\alpha)h(\alpha)d\alpha
        \end{equation*}
        If $\X(t)$ is WSS and its the input to a stable LTI system with impulse response $h(t)$, the the power spectral density of the output $\Y(t)$ is
        \begin{equation*}
            S_{yy}(\omega) = S{xx}(\omega)\abs*{H(\omega)}^2
        \end{equation*}
        where
        \begin{equation*}
            H(\omega) = \int_{-\infty}^{\infty} h(t)e^{-i\omega t}dt
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{White noise process}
        \begin{equation*}
            C_{WW}(t_1,t_2) = 0,\forall t_1 \neq t_2
        \end{equation*}
        All WSS white noise processes have $R_{WW}(t_1,t_2) = r_0\cdot \delta(t_1-t_2)$ where $r_0 = \text{constant}>0$

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Exam topics}
        \begin{itemize}
            \item LTI system, impulse response, compute Fourier transform
        \end{itemize}

    \end{multicols*}

\end{document}
