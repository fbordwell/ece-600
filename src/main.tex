\documentclass{article}
\usepackage{multicol}
\usepackage[top=1mm,bottom=1mm,left=1mm,right=1mm]{geometry}
\usepackage{lipsum}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}

\renewcommand{\S}{\mathcal{S}}
\renewcommand{\P}{\mathcal{P}}
\providecommand{\A}{\mathcal{A}}
\providecommand{\B}{\mathcal{B}}
\providecommand{\G}{\mathcal{G}}
\providecommand{\F}{\mathcal{F}}
\providecommand{\X}{\mathbb{X}}
\providecommand{\Y}{\mathbb{Y}}
\providecommand{\N}{\mathbb{N}}
\providecommand{\R}{\mathbb{R}}

\begin{document}
    \raggedright
    \small
    \setlength{\columnseprule}{0.4pt}

    \begin{multicols*}{3}

        \textbf{DeMorgan's Laws:}\\
        \begin{itemize}
            \item $\overline{A\cap B} = \overline{A} \cup \overline{B}$\\
            \item $\overline{A\cup B} = \overline{A} \cap \overline{B}$
        \end{itemize}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Countable} if elements can be put in one-to-one with $\N = \{1,2,3,\dots\}$

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Uncountable sets:}
        \begin{itemize}
            \item $\R = (-\infty, \infty)$
            \item $[0, 1] \text{ and } (0,1)$
            \item $(a,b), \forall a,b\in \R \text{ such that } a<b$
        \end{itemize}

        \noindent\rule{\columnwidth}{0.4pt}

        A family of sets $\{\A_i,i\in I\}$ is a \textbf{partition of $\S$} if it is disjoint and collectively exhaustive over $\S$.

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Axioms of Probability}
        \begin{enumerate}
            \item $\P(A)\geq0,\allowbreak\forall A\in\F(\S))$
            \item $\P(\S)=1$
            \item $\P(\cup_{i=1}^{\infty} A_i) = \sum_{i=1}^{\infty} \P( A_i)$
        \end{enumerate}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Probability Equations}
        \begin{enumerate}
            \item $\P(A\cup B) = \P(A)+\P(B)-\P(A\cap B)$
            \item $\P(A|B)=\frac{\P(A\cap B)}{\P(B)}=\frac{\P(B|A)\P(A)}{\P(B)}$
        \end{enumerate}

        \noindent\rule{\columnwidth}{0.4pt}

        Given $\S$ and a family of subsets $\G=\{\A_i, i\in I\}$ of $\S$, the \textbf{$\sigma$-field generated by $\G$}, denoted $\sigma(\G)$, is the smallest $\sigma$ field containing all the subsets in $\G$.

        By ``smallest'' $\sigma$-field, we mean that for any $\sigma$-field $\F_0$ containing all the sets in $\G$:
        \begin{equation*}
            \sigma(\G)\subset\F_0
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        Given $\R$, the \textbf{Borel field of $\R$} is defined as the $\sigma$-field generated by the family of all open intervals
        \begin{equation*}
            \G=\{(a,b):\forall(a,b)\in\R \text{ such that }a < b\}.
        \end{equation*}
        We denote the Borel field of $\R$ by $\B(\R)$.

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{pmfs}

        \textbf{uniform}\\
        $p(\omega)=\frac{1}{n},\forall\omega\in\S$\\

        \textbf{binomial}\\
        $p(k)=\allowbreak{n\choose k}a^k(1-a)^{n-k}$\\

        \textbf{geometric}\\
        $p(k)=(1-a)a^k,a\in(0,1)$\\

        \textbf{poisson}\\
        $p(k)=\frac{\lambda^{k}e^{-k}}{k!};k=0,1,2,\dots;\lambda>0$

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Properties of the pdf:}
        \begin{enumerate}
            \item $f(r)\geq0,\forall r\in\R$
            \item $\int_{-\infty}^{\infty} f(r)dr=1$
        \end{enumerate}
        Given a valid pdf, we can get a valid probability measure $\P(\cdot)$:
        \begin{equation*}
            \P(A)=\int_{A}f(r)dr=\int_{-\infty}^{\infty}\cdot1_A(r)dr
        \end{equation*}
        where
        \begin{equation*}
            1_A(r) =
            \begin{cases}
                1,r\in A\\
                0,r\notin A
            \end{cases}
        \end{equation*}
        is called the \textbf{indicator function} of the set A\@.

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{pdfs}

        \textbf{uniform}\\
        $f(r)=\frac{1}{b-a}1_{[a,b]}(r)$\\

        \textbf{exponential}\\
        $f(r)=\lambda e^{-\lambda r}\cdot 1_{[0,\infty]}(r),\lambda>0$\\
        $\hookrightarrow =
        \begin{cases}
            \lambda e^{-\lambda r},r\geq 0\\
            0,r<0
        \end{cases}$

        \textbf{gaussian}\\
        $f(r)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(\frac{-(r-\mu)^2}{2\sigma^2}\right), r\in\R,\mu\in\R,\sigma>0$

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Conditional Probability}
        \begin{equation*}
            \P(A|B)=\frac{\P(A\cap B)}{\P(B)}
        \end{equation*}

        \textbf{Bayes Formula}
        \begin{equation*}
            \P(A|B)=\frac{\P(B|A)\P(A)}{\P(B)}
        \end{equation*}

        \textbf{Total Probability Law}
        \begin{equation*}
            \P(B)=\sum_{i=1}^{n}\P(B|A_i)\P(A_i)
        \end{equation*}

        \textbf{Bayes Theorem}
        \begin{equation*}
            \P(A_m|B)=\frac{\P(B|A_m)\P(A_m)}{\sum_{i=1}^{n}\P(B|A_i)\P(A_i)}
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        Events $A$ and $B$ are \textbf{statistically independent} if and only if
        \begin{equation*}
            \P(A\cap B) = \P(A)\P(B)
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Bernoulli Trials}
        \begin{equation*}
            p_n(k)=\P(\B_k)={n\choose k}p^k(1-p)^{n-k}
        \end{equation*}
        where $p=\P_0(A)$.

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Properties of the cdf}
        \begin{equation*}
            F_{\X}(\alpha)=\P_x((-\infty,\alpha))=\P(\{\X\leq\alpha\})
        \end{equation*}
        where
        \begin{equation*}
            \{\X\leq\alpha\} = \{\omega\in\S:\X(\omega)\leq\alpha\}\in\F
        \end{equation*}
        Properties:
        \begin{enumerate}
            \item $F_\X(\infty)=1$ and $F_\X(-\infty)=0$
            \item If $x_1<x_2$, then $F_\X(x_1)\leq F_\X(x_2)$
            \item $\P(\{\X>\alpha\}) = 1-F_\X(\alpha)$
            \item If $x_1<x_2$, then $\P(\{x_1<\X\leq x_2\}) = F_\X(x_2)-F_\X(x_1)$
            \item $\P(\{\X=x_0\}) = F_\X(x_o)-F_\X(x_0^-)$ where $F_\X(x_0^-)=\lim_{\epsilon\downarrow0}F_\X(x_0-\epsilon)$
        \end{enumerate}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Properties of the pdf}
        \begin{equation*}
            f_{\X}(x)=\frac{dF_\X(x)}{dx}
        \end{equation*}
        Properties:
        \begin{enumerate}
            \item $f_\X(x)\geq0,\forall x\in\R$
            \item $F_\X(x)=\int_{-\infty}^{x} f_\X(\alpha)d\alpha $
            \item $\int_{\infty}^{\infty} f_\X(x)dx = F_\X(\infty) - F_\X(-\infty) = 1$
            \item $\P(\{x_1<\X\leq x_2\}) =\int_{x_1}^{x_2}f_\X(x)dx=F_\X(x_2)-F_\X(x_1) $
        \end{enumerate}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Dirac $\delta$-functions:} $\delta(x)$

        \begin{itemize}
            \item $\delta(x)=0,\forall x\neq0$
            \item $\int_{\infty}^{\infty} \delta(x)dx=\int_{-\epsilon}^{\epsilon}\delta(x)dx=1,\forall\epsilon>0 $
        \end{itemize}

        \textbf{Sifting Property of Dirac $\delta$-functions}
        \begin{equation*}
            \int_{\infty}^{\infty} g(x)\delta(x-x_0)dx=g(x_0)
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Random Variable Forms}

        \textbf{gaussian}
        \begin{equation*}
            f_\X(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp[\frac{-(x-\mu)^2}{2\sigma^2}],\forall x\in\R
        \end{equation*}
        n.b.
        \begin{equation*}
            \begin{gathered}
                F_\X(x)=\int_{-\infty}^{x}f_{\X}(\alpha)d\alpha=\Phi(\frac{x-\mu}{\sigma})\\
                \text{where } \Phi(r)=\int_{-\infty}^{r} \frac{1}{\sqrt{2\pi}}e^{\frac{-z^2}{2}}dz
            \end{gathered}
        \end{equation*}
        So if $\X$ is a Gaussian RV with parameters $\mu\in\R$ and $\sigma>0$, then
        \begin{equation*}
            \P(\{a<\X\leq b\}) = \Phi(\frac{b-\mu}{\sigma}) - \Phi(\frac{a-\mu}{\sigma})
        \end{equation*}

        \textbf{uniform}
        \begin{equation*}
            \begin{gathered}
                \X=\text{u}[a,b], a<b\\
                f_\X(x)=\frac{1}{b-a}\cdot 1_{[a,b]}(x)\\
                F_{\X}(x)=\int_{\infty}^{x} f_{\X}(\alpha)d\alpha
            \end{gathered}
        \end{equation*}

        \textbf{exponential}\\
        An RV $\X$ with pdf of the form
        \begin{equation*}
            f_{\X}(x)=\alpha e^{-\alpha x}\cdot 1_{[0,\infty]}(x)
        \end{equation*}
        where $\alpha>0$ is called an exponential RV with parameter $\alpha$.

        \textbf{binomial}\\
        Discrete RV taking on values in the set $\{0,1,2,\cdots,n\}\subset\R$ with pmf
        \begin{equation*}
            \begin{gathered}
                \P_{\X}(k)={n\choose k}p^k(1-p)^{n-k},\\
                F_{\X}(x)= \sum_{k=0}^{m(x)} {n\choose k}p^k(1-p)^{n-k}\\
                \text{where } m(x)\leq x< m(x)+1,\\
                f_{\X}(x) = \sum_{k=0}^{n} {n\choose k}p^k(1-p)^{n-k}\delta(x-k)
            \end{gathered}
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Conditional cdf and pdf}
        \begin{equation*}
            \begin{gathered}
                F_{\X}(x|M)=\P(\{\X\leq x\}|M) = \frac{\P(\{\X\leq x\}\cap M)}{\P(M)}\\
                f_{\X}(x|M) = \frac{dF_\X(x|M)}{dx}
            \end{gathered}
        \end{equation*}

        \noindent\rule{\columnwidth}{0.4pt}

        \textbf{Total Probability Law}
        \begin{equation*}
            \begin{gathered}
                F_{\X}(x)=F_{\X}(x|A_1)P(A_1)+\cdots+F_{\X}(x|A_n)P(A_n)\\
                f_{\X}(x)=f_{\X}(x|A_1)P(A_1)+\cdots+f_{\X}(x|A_n)P(A_n)
            \end{gathered}
        \end{equation*}

        \textbf{Bayes Formula}
        \begin{equation*}
            \begin{gathered}
                \P(A|\{\X\leq x\}) = \frac{F_\X(x|A)\P(A)}{F_\X(x)}
            \end{gathered}
        \end{equation*}

        \newpage
        Now consider $\P(A|B) = \frac{\P(B|A)\P(A)}{\P(B)}$ when $B=\{x_1<\X\leq x_2\}$:
        \begin{equation*}
            \begin{gathered}
                \P(A|\{x_1<\X\leq x_2\}) = \\
                \hookrightarrow = \frac{F_\X (x_2|A) - F_\X (x_1|A)\P(A)}{F_\X(x_2) - F_\X(x_1)}
            \end{gathered}
        \end{equation*}

        For $\X=x$,
        \begin{equation*}
            \begin{gathered}
                \P(A|\{\X=x\}) = \frac{f_\X(x|A)}{f_\X(x)}\P(A)\\
                \P(A)= \int_{-\infty}^{\infty} \P(A|\{\X=x\})f_{\X}dx
            \end{gathered}
        \end{equation*}

        \textbf{Bayes' Theorem}
        \begin{equation*}
            f_{\X}(x|A) = \frac{\P(A|\{\X=x\})f_\X(x)}{\int_{-\infty}^{\infty}\P(A|\{\X=\alpha\})f_\X(\alpha)d\alpha }
        \end{equation*}

    \end{multicols*}

\end{document}
